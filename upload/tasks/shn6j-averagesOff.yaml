description: >-
  Write a function named `averagesOff` which, given two values, determines their
  absolute difference divided by their average. However, if their difference is
  less than 1, assume that it is 1. If their average is less than 1, assume that
  it is 1. For instance, `averagesOff(-4,-2)` would assume an average of -3 and
  a difference of 1, and therefore would return 1/(-3)=-0.3333333333333333.
  `averagesOff(4,4.5)` would see an average of 4.25 and a difference of .5,
  which it would assume is 1, so it would return 1/4.25=0.23529411764705882.
topics: [conditionals]
solution: |-
  #Patched description errors
  def averagesOff(a,b):
    avg=(a+b)/2
    if avg<1:
      avg=1
    diff=abs(a-b)
    if diff<1:
      diff=1
    return diff/avg
func: averagesOff
args:
  - [2, 0]
  - [2, 2]
  - [2, 2.2]
  - [-4, -4.1]
  - [-4, -6]
  - [4, hi]
accepted: expect an email from Prof T about picking up your gift card
